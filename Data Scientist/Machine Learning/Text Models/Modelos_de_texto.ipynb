{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOdelos de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Stemming | Lemmatization|\n",
    "|---|---|\n",
    "| Use `fixed rules such as able,ing` to derive a base word | Use `knowledge of a language` (a.k.a. linguistic knowledge) to derive a word|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jm_he\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"All models are wrong, BUT some ARE usefully.\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"All models are wrong, BUT some ARE usefully.\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all model are wrong , but some are usefully .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(lemmas)\n",
    "# lemmatizer.lemmatize('profitable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all model be wrong , but some be usefully for the traditional .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "text = \"All models are wrong, BUT some ARE usefully for the traditional.\"\n",
    "nlp = spacy.load('en_core_web_sm', disable = ['parser', 'ner'])\n",
    "doc = nlp(text.lower())\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "print(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm',disable=['parser','ner'])\n",
    "\n",
    "data = pd.read_csv('D:/Tripleten/datasets/imdb_reviews_small.tsv', sep='\\t')\n",
    "corpus = data['review']\n",
    "phrases = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4279,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    for phrase in text:\n",
    "        doc = nlp(phrase.lower())\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        phrases.append(\" \".join(lemmas))\n",
    "\n",
    "lemmatize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expresiones Regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I liked this show from the first episode I saw, which was the \"Rhapsody in Blue\" episode (for those that don't know what that is, the Zan going insane and becoming pau lvl 10 ep). Best visuals and special effects I've seen on a television series, nothing like it anywhere.\n",
      "I liked this show from the first episode I saw which was the Rhapsody in Blue episode for those that don't know what that is the Zan going insane and becoming pau lvl ep Best visuals and special effects I've seen on a television series nothing like it anywhere\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = '''I liked this show from the first episode I saw, which was the \"Rhapsody in Blue\" episode (for those that don't know what that is, the Zan going insane and becoming pau lvl 10 ep). Best visuals and special effects I've seen on a television series, nothing like it anywhere.'''\n",
    "# re.sub(pattern, substitution, text)\n",
    "\n",
    "clear_text = re.sub(r'[^A-Za-z\\' ]', '', text)\n",
    "clear_text = \" \".join(clear_text.split())  # es muy importante aplicar el split join porque algunas veces se quedan dobles espacios por los elementos que estamos descartando\n",
    "    \n",
    "print(text)\n",
    "print(clear_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words\n",
    "\n",
    "Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'for': 3, 'want': 3, 'of': 3, 'a': 3, 'the': 3, 'be': 3, 'lose': 3, 'shoe': 2, 'horse': 2, 'nail': 1, 'rider': 1})\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser','ner'])\n",
    "text =  \"\"\"For want of a nail the shoe was lost. For want of a' shoe the horse was \"lost\". For want of a horse the rider was lost.\"\"\"\n",
    "doc = nlp(text)\n",
    "tokens = [token.lemma_ for token in doc if not token.is_punct] #token.is_punct excluye los signos de puntuacion.\n",
    "bow = Counter(tokens) #Cuenta el número de cada palabra,\n",
    "\n",
    "vector = [bow[token] for token in sorted(bow)]\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a bag of wwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tamaño (7, 16): 7 textos, 16 palabras únicas\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer() #Count vecotizer no toma en cuenta letras individuales.\n",
    "\n",
    "corpus = [\n",
    "    'for want of a nail the shoe be lose',\n",
    "    'for want of a shoe the horse be lose',\n",
    "    'for want of a horse the rider be lose',\n",
    "    'for want of a rider the message be lose',\n",
    "    'for want of a message the battle be lose',\n",
    "    'for want of a battle the kingdom be lose',\n",
    "    'and all for the want of a horseshoe nail'\n",
    "]\n",
    "\n",
    "bow = count_vect.fit_transform(corpus)\n",
    "\n",
    "# print(bow)\n",
    "print(f' Tamaño {bow.shape}: {bow.shape[0]} textos, {bow.shape[1]} palabras únicas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1]\n",
      " [0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1]\n",
      " [0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 1]\n",
      " [0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1]\n",
      " [0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1]\n",
      " [0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1]\n",
      " [1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['all', 'and', 'battle', 'be', 'for', 'horse', 'horseshoe',\n",
       "       'kingdom', 'lose', 'message', 'nail', 'of', 'rider', 'shoe', 'the',\n",
       "       'want'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# En cualquier momento podemos transformar estos valores a un array.\n",
    "print(bow.toarray())\n",
    "\n",
    "#He imprimir sus valores para consulta\n",
    "count_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si quisieramos hacer una bolsa de palabras con 2 o más elementos podemos usar el parametro ngram_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "phrase = ('Perros locos de wisconsin', 'Gatos locos de machupichu')\n",
    "count_vect = CountVectorizer(ngram_range=(2,2)) # Si quisieramos hacer bigramas (Mínimo y máximo)\n",
    "bow = count_vect.fit_transform(phrase)\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['bird has', 'get going', 'has landed', 'is happening', 'it is'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "phrase = ['It is happening.', 'Bird has landed.', 'Get going' ]\n",
    "count_vect = CountVectorizer(ngram_range=(2,2)) \n",
    "bow = count_vect.fit_transform(phrase)\n",
    "print(bow.shape)\n",
    "count_vect.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas veces deseamos excluir palabras vacias (que por si solas no aportan nada), para poder excluirlas podemos importarlas de la libreria de nltk y mandarselas como parametro a nuestro CounVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tamaño (7, 10): 7 textos, 10 palabras únicas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jm_he\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['battle', 'horse', 'horseshoe', 'kingdom', 'lose', 'message',\n",
       "       'nail', 'rider', 'shoe', 'want'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "count_vect = CountVectorizer(stop_words=stop_words) #Count vectizer no toma en cuenta letras individuales.\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    'for want of a nail the shoe be lose',\n",
    "    'for want of a shoe the horse be lose',\n",
    "    'for want of a horse the rider be lose',\n",
    "    'for want of a rider the message be lose',\n",
    "    'for want of a message the battle be lose',\n",
    "    'for want of a battle the kingdom be lose',\n",
    "    'and all for the want of a horseshoe nail'\n",
    "]\n",
    "\n",
    "bow = count_vect.fit_transform(corpus)\n",
    "\n",
    "print(f' Tamaño {bow.shape}: {bow.shape[0]} textos, {bow.shape[1]} palabras únicas')\n",
    "count_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La lista de palabras únicas en la bolsa se llama vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12385164096708581"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "math.log10(1.33)\n",
    "\n",
    "df = pd.DataFrame([1,2,3,4])\n",
    "df.counkz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IFD \n",
    "La formula TF-IDF mejor conocida como Term Frequency , Inverse Document Frequency, se ejecuta de la siguiente forma.\n",
    "\n",
    "$$ TF-IDF  - TF * IDF $$\n",
    "\n",
    "En donde: \n",
    "\n",
    "$TF = \\frac{f}{n}$\n",
    "\n",
    "- f = frecuencia que aparece la palabra\n",
    "- n = numero total de palabras\n",
    "\n",
    "$IDF = log_{10}(\\frac{D}{d})$\n",
    "\n",
    "- D = Numero de parrafos\n",
    "- d = Numero de parrafos en donde aparece la palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small_lemm.tsv', sep='\\t')\n",
    "corpus = data['review_lemm']\n",
    "\n",
    "stop_words = set(nltk_stopwords.words('english'))\n",
    "vect = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "tf_idf = vect.fit_transform(corpus)\n",
    "\n",
    "print('El tamaño de la matriz TF-IDF:', tf_idf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente ejercicio incluye el uso de creación de bolas para ser usadas en un modelo de regresion. Las bolsas por si solas no mencionan si algo es negativo o positivo es por eso que la columna [pos] entrenará el modelo y le enseñara que elementos han sido considerados como negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El resultado accuracy_score es 0.8757396449704142\n",
      "El resultado f1_score es 0.9007874015748032\n",
      "El resultado roc_score es 0.857286576602472\n",
      "      pos\n",
      "0       0\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       0\n",
      "...   ...\n",
      "2215    0\n",
      "2216    1\n",
      "2217    1\n",
      "2218    1\n",
      "2219    1\n",
      "\n",
      "[2220 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, root_mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_data = pd.read_csv('D:/Tripleten/datasets/imdb_reviews_small_lemm_train.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('D:/Tripleten/datasets/imdb_reviews_small_lemm_test.tsv', sep='\\t')\n",
    "\n",
    "train_corpus = train_data['review_lemm']# extraer reseñas lematizadas para el entrenamiento\n",
    "\n",
    "stop_words = list(set(nltk_stopwords.words('english')))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "tf_idf = count_tf_idf.fit_transform(train_corpus)\n",
    "\n",
    "features_train = tf_idf # Definir elementros\n",
    "target_train = train_data['pos']# extraer la columna objetivo\n",
    "\n",
    "test_corpus = test_data['review_lemm']# extraer reseñas lematizadas para la prueba\n",
    "features_test = count_tf_idf.transform(test_corpus) # transformar el corpus de entrenamiento\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(features_train, target_train ,test_size=1/4 , random_state=12345)\n",
    "\n",
    "model =  LogisticRegression() # inicializar el modelo de regresión logística y ajustarlo\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "pred_test = model.predict(X_test)# obtener predicciones para la parte de prueba de los datos\n",
    "acc_score = accuracy_score(y_test,pred_test) \n",
    "f1score = f1_score(y_test,pred_test)\n",
    "roc_score = roc_auc_score(y_test,pred_test)\n",
    "# rmse = root_mean_squared_error(y_test,pred_test) #No es usado para clasificación\n",
    "\n",
    "\n",
    "print(f'El resultado accuracy_score es {acc_score}')\n",
    "print(f'El resultado f1_score es {f1score}')\n",
    "print(f'El resultado roc_score es {roc_score}')\n",
    "# print(f'El resultado rmse es {rmse}')\n",
    "\n",
    "\n",
    "pred_test = model.predict(features_test)# obtener predicciones para la parte de prueba de los datos\n",
    "\n",
    "# transformar las predicciones en un DataFrame y guardarlo\n",
    "submission = pd.DataFrame({'pos':pred_test})\n",
    "\n",
    "print(submission)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
