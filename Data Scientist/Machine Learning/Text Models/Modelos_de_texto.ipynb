{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOdelos de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Stemming | Lemmatization|\n",
    "|---|---|\n",
    "| Use `fixed rules such as able,ing` to derive a base word | Use `knowledge of a language` (a.k.a. linguistic knowledge) to derive a word|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jm_he\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"All models are wrong, BUT some ARE usefully.\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"All models are wrong, BUT some ARE usefully.\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all model are wrong , but some are usefully .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(lemmas)\n",
    "# lemmatizer.lemmatize('profitable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all model be wrong , but some be usefully for the traditional .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "text = \"All models are wrong, BUT some ARE usefully for the traditional.\"\n",
    "nlp = spacy.load('en_core_web_sm', disable = ['parser', 'ner'])\n",
    "doc = nlp(text.lower())\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "print(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm',disable=['parser','ner'])\n",
    "\n",
    "data = pd.read_csv('D:/Tripleten/datasets/imdb_reviews_small.tsv', sep='\\t')\n",
    "corpus = data['review']\n",
    "phrases = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4279,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    for phrase in text:\n",
    "        doc = nlp(phrase.lower())\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        phrases.append(\" \".join(lemmas))\n",
    "\n",
    "lemmatize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expresiones Regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I liked this show from the first episode I saw, which was the \"Rhapsody in Blue\" episode (for those that don't know what that is, the Zan going insane and becoming pau lvl 10 ep). Best visuals and special effects I've seen on a television series, nothing like it anywhere.\n",
      "I liked this show from the first episode I saw which was the Rhapsody in Blue episode for those that don't know what that is the Zan going insane and becoming pau lvl ep Best visuals and special effects I've seen on a television series nothing like it anywhere\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = '''I liked this show from the first episode I saw, which was the \"Rhapsody in Blue\" episode (for those that don't know what that is, the Zan going insane and becoming pau lvl 10 ep). Best visuals and special effects I've seen on a television series, nothing like it anywhere.'''\n",
    "# re.sub(pattern, substitution, text)\n",
    "\n",
    "clear_text = re.sub(r'[^A-Za-z\\' ]', '', text)\n",
    "clear_text = \" \".join(clear_text.split())  # es muy importante aplicar el split join porque algunas veces se quedan dobles espacios por los elementos que estamos descartando\n",
    "    \n",
    "print(text)\n",
    "print(clear_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words\n",
    "\n",
    "Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser','ner'])\n",
    "text =  \"\"\"For want of a nail the shoe was lost. For want of a' shoe the horse was \"lost\". For want of a horse the rider was lost.\"\"\"\n",
    "doc = nlp(text)\n",
    "tokens = [token.lemma_ for token in doc if not token.is_punct] #token.is_punct excluye los signos de puntuacion.\n",
    "bow = Counter(tokens) #Cuenta el número de cada palabra,\n",
    "\n",
    "vector = [bow[token] for token in sorted(bow)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'for': 3,\n",
       "         'want': 3,\n",
       "         'of': 3,\n",
       "         'a': 3,\n",
       "         'the': 3,\n",
       "         'be': 3,\n",
       "         'lose': 3,\n",
       "         'shoe': 2,\n",
       "         'horse': 2,\n",
       "         'nail': 1,\n",
       "         'rider': 1})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a bag of wwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 8)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 5)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 12)\t1\n",
      "  (3, 4)\t1\n",
      "  :\t:\n",
      "  (3, 9)\t1\n",
      "  (4, 4)\t1\n",
      "  (4, 15)\t1\n",
      "  (4, 11)\t1\n",
      "  (4, 14)\t1\n",
      "  (4, 3)\t1\n",
      "  (4, 8)\t1\n",
      "  (4, 9)\t1\n",
      "  (4, 2)\t1\n",
      "  (5, 4)\t1\n",
      "  (5, 15)\t1\n",
      "  (5, 11)\t1\n",
      "  (5, 14)\t1\n",
      "  (5, 3)\t1\n",
      "  (5, 8)\t1\n",
      "  (5, 2)\t1\n",
      "  (5, 7)\t1\n",
      "  (6, 4)\t1\n",
      "  (6, 15)\t1\n",
      "  (6, 11)\t1\n",
      "  (6, 10)\t1\n",
      "  (6, 14)\t1\n",
      "  (6, 1)\t1\n",
      "  (6, 0)\t1\n",
      "  (6, 6)\t1\n",
      " Tamaño (7, 16): 7 textos, 16 palabras únicas\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer() #Count vecotizer no toma en cuenta letras individuales.\n",
    "\n",
    "corpus = [\n",
    "    'for want of a nail the shoe be lose',\n",
    "    'for want of a shoe the horse be lose',\n",
    "    'for want of a horse the rider be lose',\n",
    "    'for want of a rider the message be lose',\n",
    "    'for want of a message the battle be lose',\n",
    "    'for want of a battle the kingdom be lose',\n",
    "    'and all for the want of a horseshoe nail'\n",
    "]\n",
    "\n",
    "bow = count_vect.fit_transform(corpus)\n",
    "\n",
    "print(bow)\n",
    "print(f' Tamaño {bow.shape}: {bow.shape[0]} textos, {bow.shape[1]} palabras únicas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1]\n",
      " [0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1]\n",
      " [0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 1]\n",
      " [0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1]\n",
      " [0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1]\n",
      " [0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1]\n",
      " [1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['all', 'and', 'battle', 'be', 'for', 'horse', 'horseshoe',\n",
       "       'kingdom', 'lose', 'message', 'nail', 'of', 'rider', 'shoe', 'the',\n",
       "       'want'], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(2,2)) # Si quisieramos hacer bigramas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jm_he\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "count_vect = CountVectorizer(stop_words=stop_words) #Count vectizer no toma en cuenta letras individuales.\n",
    "\n",
    "corpus = [\n",
    "    'for want of a nail the shoe be lose',\n",
    "    'for want of a shoe the horse be lose',\n",
    "    'for want of a horse the rider be lose',\n",
    "    'for want of a rider the message be lose',\n",
    "    'for want of a message the battle be lose',\n",
    "    'for want of a battle the kingdom be lose',\n",
    "    'and all for the want of a horseshoe nail'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 7)\t1\n",
      "  (3, 9)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 5)\t1\n",
      "  (4, 9)\t1\n",
      "  (4, 4)\t1\n",
      "  (4, 5)\t1\n",
      "  (4, 0)\t1\n",
      "  (5, 9)\t1\n",
      "  (5, 4)\t1\n",
      "  (5, 0)\t1\n",
      "  (5, 3)\t1\n",
      "  (6, 9)\t1\n",
      "  (6, 6)\t1\n",
      "  (6, 2)\t1\n",
      "\n",
      " Tamaño (7, 10): 7 textos, 10 palabras únicas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['battle', 'horse', 'horseshoe', 'kingdom', 'lose', 'message',\n",
       "       'nail', 'rider', 'shoe', 'want'], dtype=object)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bow = count_vect.fit_transform(corpus)\n",
    "\n",
    "print(bow)\n",
    "print()\n",
    "print(f' Tamaño {bow.shape}: {bow.shape[0]} textos, {bow.shape[1]} palabras únicas')\n",
    "count_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lista de palabras únicas en la bolsa se llama vocabulario"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
