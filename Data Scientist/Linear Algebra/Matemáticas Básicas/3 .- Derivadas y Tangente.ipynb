{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivadas $f'(x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una derivada es una medida de cómo cambia una función en respuesta a pequeños cambios en su variable independiente. En términos más simples, la derivada de una función te dice cómo cambia el valor de esa función a medida que cambia el valor de su entrada.\n",
    "\n",
    "Para una función $f(x)$, la derivada $f'(x)$ (también escrita como $\\frac{{df}}{{dx}}$) representa la tasa de cambio instantánea de $f$ con respecto a $x$. Puedes pensar en esto como la pendiente de la tangente a la curva de la función en un punto dado.\n",
    "\n",
    "Por ejemplo, si tienes una función que representa la posición de un objeto en función del tiempo, su derivada con respecto al tiempo te daría la velocidad del objeto en cada momento dado. Si tomas la derivada de la velocidad, obtendrías la aceleración, que representa cómo cambia la velocidad en función del tiempo.\n",
    "\n",
    "La derivada también puede interpretarse geométricamente como la pendiente de la recta tangente a la curva en un punto dado. Una derivada positiva indica que la función está aumentando en ese punto, una derivada negativa indica que está disminuyendo, y una derivada cero indica un punto estacionario o un máximo/mínimo local.\n",
    "\n",
    "Las derivadas son fundamentales en el cálculo y tienen numerosas aplicaciones en física, ingeniería, economía y muchas otras áreas donde se estudian fenómenos que cambian continuamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*x\n",
      "4*y + 10*z\n",
      "10*y - 5\n"
     ]
    }
   ],
   "source": [
    "import sympy as sym\n",
    "\n",
    "x, y, z = sym.symbols('x y z')\n",
    "\n",
    "# Define the function f\n",
    "f = x**2 + 2*y**2 + 10*y*z - 5*z\n",
    "\n",
    "derivada_x = sym.diff(f,x)\n",
    "derivada_y = sym.diff(f,y)\n",
    "derivada_z = sym.diff(f,z)\n",
    "\n",
    "print(derivada_x)\n",
    "print(derivada_y)\n",
    "print(derivada_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradiente $\\nabla$\n",
    "\n",
    "La \"gradiente\" es un concepto que se aplica específicamente a funciones de múltiples variables. Cuando trabajas con una sola variable, usas el término \"derivada\" para indicar cómo cambia la función con respecto a esa variable.\n",
    "\n",
    "Derivada: Se refiere al cambio instantáneo de una función de` una sola variable` respecto a esa variable.\n",
    "Gradiente: Se refiere al vector que contiene las derivadas parciales de una función de `múltiples variables` respecto a cada una de esas variables.\n",
    "\n",
    "\n",
    "\n",
    "Encuentra el gradiente de una función de dos argumentos:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4*x - 5, 3]\n"
     ]
    }
   ],
   "source": [
    "import sympy as sym\n",
    "\n",
    "x, y= sym.symbols('x y')\n",
    "\n",
    "# Define the function f\n",
    "f = 2*x**2 + 3*y - 5*x + 20\n",
    "\n",
    "\n",
    "derivada_x = sym.diff(f,x)\n",
    "derivada_y = sym.diff(f,y)\n",
    "\n",
    "\n",
    "gradiente = ([derivada_x, derivada_y])\n",
    "\n",
    "print(gradiente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encuentra el gradiente de una función de tres argumentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6*x + 3*y, 3*x + 4*y, -3]\n"
     ]
    }
   ],
   "source": [
    "import sympy as sym\n",
    "\n",
    "x, y, z = sym.symbols('x y z')\n",
    "\n",
    "f = 3*x**2 + 2*y**2 + 3*x*y -3*z\n",
    "\n",
    "\n",
    "derivative_x = sym.diff(f,x)\n",
    "derivative_y = sym.diff(f,y)\n",
    "derivative_z = sym.diff(f,z)\n",
    "\n",
    "print( [derivative_x,derivative_y, derivative_z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2*x - 10]\n"
     ]
    }
   ],
   "source": [
    "import sympy as sym\n",
    "\n",
    "x= sym.symbols('x')\n",
    "\n",
    "# Define the function f\n",
    "f = (x-5)**2\n",
    "\n",
    "\n",
    "derivada_x = sym.diff(f,x)\n",
    "\n",
    "gradiente = ([derivada_x])\n",
    "\n",
    "print(gradiente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descenso de gradiente\n",
    "\n",
    "El descenso de gradiente es un algoritmo de optimización utilizado para encontrar el mínimo de una función. Es particularmente útil en el contexto de problemas de optimización, como el entrenamiento de modelos de aprendizaje automático, donde queremos minimizar una función de pérdida.\n",
    "\n",
    "El objetivo del descenso de gradiente es encontrar el mínimo local o global de una función f(x) mediante iteraciones sucesivas, donde en cada iteración ajustamos el valor de x en la dirección y magnitud adecuadas para reducir la función f(x) lo más posible.\n",
    "\n",
    "El algoritmo funciona de la siguiente manera:\n",
    "\n",
    "1. Comenzamos con un punto inicial $x^0$.\n",
    "2. Calculamos el gradiente de la función f en el punto $x^0$, que nos indica la dirección y magnitud del mayor cambio positivo de la función en ese punto.\n",
    "3. Ajustamos x^0 en la dirección opuesta al gradiente multiplicado por un tamaño de paso μ (llamado tasa de aprendizaje o tamaño de paso), para obtener el siguiente punto $x^1$.\n",
    "4. Repetimos el proceso desde el paso 2 con el nuevo punto $x^1$, hasta que se cumpla algún criterio de detención, como un número máximo de iteraciones o una tolerancia en la convergencia.\n",
    "\n",
    "La fórmula general para el descenso de gradiente es:\n",
    "\n",
    "$x^{n+1} = x^n - \\mu \\nabla f(x^n)$\n",
    "\n",
    "Donde:\n",
    "- $x^n$ es el punto en la n-ésima iteración.\n",
    "- $x^{n+1}$ es el punto en la (n+1)-ésima iteración.\n",
    "- $\\mu $ es el tamaño de paso (tasa de aprendizaje).\n",
    "- $ \\nabla f(x^n)$ es el gradiente de la función f evaluado en el punto x^n.\n",
    "\n",
    "La formula de descenso de gradiente a traves de matrices es:\n",
    "\n",
    "$\\nabla ECM (Xw , y) = \\frac{2}{n}X^T(Xw-y) $ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2*x - 20]\n"
     ]
    }
   ],
   "source": [
    "import sympy as sym\n",
    "\n",
    "x = sym.symbols('x')\n",
    "\n",
    "f = (x-10)**2\n",
    "\n",
    "derivative_x = sym.diff(f,x)\n",
    "\n",
    "print( [derivative_x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración 1: x = 8.0 y loss = 100.000000\n",
      "Iteración 2: x = 9.6 y loss = 4.000000\n",
      "Iteración 3: x = 9.92 y loss = 0.160000\n",
      "Iteración 4: x = 9.984 y loss = 0.006400\n",
      "Iteración 5: x = 9.9968 y loss = 0.000256\n",
      "Iteración 6: x = 9.99936 y loss = 0.000010\n",
      "Iteración 7: x = 9.999872 y loss = 0.000000\n",
      "Valor final de x:10.00\n",
      "Valor final de perdida:0.0000004096\n",
      "Counts:7\n"
     ]
    }
   ],
   "source": [
    "# Funcion\n",
    "\n",
    "def f(x):\n",
    "    return (x-10)**2\n",
    "\n",
    "# Derivada\n",
    "\n",
    "def df(x):\n",
    "    return 2*(x-10)\n",
    "\n",
    "# Tamaño del paso\n",
    "mu = 0.4\n",
    "\n",
    "# Valor inicial de x\n",
    "x = 0\n",
    "count = 0\n",
    "loss= 1\n",
    "\n",
    "while loss > 0.00001:\n",
    "    loss= f(x)\n",
    "    x = x- mu * df(x)\n",
    "    print(f'Iteración {count+1}: x = {x} y loss = {loss:.6f}')\n",
    "    count += 1\n",
    "\n",
    "print(f'Valor final de x:{x:.2f}')\n",
    "print(f'Valor final de perdida:{loss:.10f}')\n",
    "print(f'Counts:{count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración 1: x = 8.0, pérdida = 100\n",
      "Iteración 2: x = 9.6, pérdida = 4.0\n",
      "Iteración 3: x = 9.92, pérdida = 0.16000000000000028\n",
      "Iteración 4: x = 9.984, pérdida = 0.006400000000000012\n",
      "Valor final de x: 9.984\n",
      "Valor final de la función de pérdida: 0.0002560000000000005\n"
     ]
    }
   ],
   "source": [
    "# Definición de la función de pérdida\n",
    "def f(x):\n",
    "    return (x - 10) ** 2\n",
    "\n",
    "# Derivada de la función de pérdida\n",
    "def df(x):\n",
    "    return 2 * (x - 10)\n",
    "\n",
    "# Tamaño del paso\n",
    "mu = 0.4\n",
    "\n",
    "# Valor inicial de x\n",
    "x = 0\n",
    "\n",
    "\n",
    "# Iteraciones de descenso de gradiente\n",
    "for i in range(4):\n",
    "    # Calcula el valor de la función de pérdida\n",
    "    loss = f(x)\n",
    "    \n",
    "    # Actualiza x utilizando el descenso de gradiente\n",
    "    x = x - mu * df(x)\n",
    "    \n",
    "    # Imprime el valor de x y de la función de pérdida en esta iteración\n",
    "    print(f\"Iteración {i+1}: x = {x}, pérdida = {loss}\")\n",
    "\n",
    "# Imprime el resultado final\n",
    "print(\"Valor final de x:\", x)\n",
    "print(\"Valor final de la función de pérdida:\", f(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4*x - 6, 4*y + 2]\n"
     ]
    }
   ],
   "source": [
    "import sympy as sym\n",
    "\n",
    "x, y = sym.symbols('x y')\n",
    "\n",
    "f = (x+y-1)**2 + (x-y-2)**2\n",
    "\n",
    "derivada_x = sym.diff(f,x)\n",
    "derivada_y = sym.diff(f,y)\n",
    "\n",
    "derivada = [derivada_x,derivada_y]\n",
    "\n",
    "print(derivada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6  2]\n",
      "[-4.   3.2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def func(x):\n",
    "    return (x[0] + x[1] - 1) ** 2 + (x[0] - x[1] - 2) ** 2\n",
    "\n",
    "\n",
    "def gradient(x):\n",
    "    return np.array([4 * x[0] - 6, 4* x[1] + 2])\n",
    "\n",
    "        \n",
    "print(gradient([0, 0]))\n",
    "print(gradient(np.array([0.5, 0.3])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[41.2],\n",
       "       [76. ],\n",
       "       [61.4],\n",
       "       [83.8]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "X = np.array([[4,5,2,3],[2,5,2,5],[2,5,7,8]])\n",
    "y = np.array([0,1,0]).reshape(3,-1)\n",
    "w = np.array([.6,.4,.2,.2]).reshape(4,-1)\n",
    "\n",
    "X.T @ (X @ w -y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 4., 5., 2., 3.],\n",
       "       [1., 1., 1., 2., 5., 2., 5.],\n",
       "       [1., 1., 1., 2., 5., 7., 8.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_conc = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "    #  X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "X_conc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 4., 5., 2., 3.],\n",
       "       [0., 1., 1., 2., 5., 2., 5.],\n",
       "       [0., 1., 1., 2., 5., 7., 8.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.concatenate(np.ones((X.shape[0],1)),X, axis=1)\n",
    "\n",
    "\n",
    "nb = np.ones(((X.shape[0],1)))\n",
    "\n",
    "X_conc = np.concatenate((nb,X),axis=1)\n",
    "\n",
    "X_conc\n",
    "\n",
    "np.concatenate(((np.zeros((X.shape[0],1))), X) , axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo DGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Descenso de Gradiente Estocástico (SGD, por sus siglas en inglés) es un algoritmo de optimización utilizado en el aprendizaje automático para entrenar modelos. Es una variante del método de descenso de gradiente clásico que utiliza una sola muestra de datos (o un pequeño lote de muestras) seleccionadas al azar en cada paso de iteración, en lugar de usar todo el conjunto de datos para calcular el gradiente.\n",
    "\n",
    "El algoritmo SGD es útil cuando se trabaja con conjuntos de datos grandes, ya que el cálculo del gradiente en cada paso solo requiere una pequeña fracción del conjunto de datos. Además, la aleatoriedad introducida por el muestreo estocástico puede ayudar al algoritmo a escapar de mínimos locales y alcanzar mínimos globales más rápidamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Supongamos que tienes tus datos de entrenamiento en X_train y y_train,\n",
    "# y los datos de prueba en X_test y y_test.\n",
    "\n",
    "# Crear un pipeline que estandarice los datos y luego aplique SGDRegressor\n",
    "model = make_pipeline(StandardScaler(), SGDRegressor(max_iter=1000, tol=1e-3))\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "predictions_train = model.predict(X_train)\n",
    "predictions_test = model.predict(X_test)\n",
    "\n",
    "# Calcular el error cuadrático medio\n",
    "mse_train = mean_squared_error(y_train, predictions_train)\n",
    "mse_test = mean_squared_error(y_test, predictions_test)\n",
    "\n",
    "print(\"Error cuadrático medio en conjunto de entrenamiento:\", mse_train)\n",
    "print(\"Error cuadrático medio en conjunto de prueba:\", mse_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
